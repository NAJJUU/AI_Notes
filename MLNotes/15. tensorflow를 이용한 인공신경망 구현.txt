* Deep Learning으로 정교하게 예측하기
    1) 인공신경망
        - 다층 퍼셉트론(MLP)
            -입력층, 출력층, 은닉층
        - 경사하강법
            -오류역전파(backpropagation)
    2) 신경망 구조
        -입력층
         -은닉층
            -ReLU함수, 탄젠트 함수
        -출력층
            -Softmax 분류기
    3) Deep Learning

* 인공신경망(ANN)
    -다층 퍼셉트론(Multi-Layer Perceptron)
        -입력층 하나와 은닉층(Hidden Layer)이라 불리는 하나 이상의 층과 출력층으로 구성
        -출력층을 제외하고 모든 층은 편향 뉴런을 포함하여 다음층과 완전히 연결
        -인공신경망의 은닉층이 2개 이상일 때 심층 신경망(DNN)이라 부름
    -역전파(Backpropagation)
        -다층 퍼셉트론을 효율적으로 학습시킬 수 있는 알고리즘
        -각 훈련 샘플(데이터셋)에 대해 역전파 알고리즘이 먼저 예측을 만들고, 오차를 측정하고(정방향),
         그 다음 역방향으로 각 층을 거치면서 각 연결이 오차에 기여한 정도를 측정(역방향)
        -이 오차가 감소하도록 가중치(weight)를 조금씩 조정 <== 경사하강법
            
* Activation function
    -활성화 함수
        -딥러닝 네트워크에는 노드에 들어오는 값들에 대해 곧바로 다음 레이어로 전달하지 않고
         주로 비선형 함수를 통과시킨 후 전달함
        -이때 사용하는 함수를 활성화 함수라 부름.
    -은닉층
        -하이퍼볼릭 탄젠트(tanh) 함수
            -시그노이드(Sigmoid) 함수를 보완
            -출력범위가 -1~1 사이 => 각층의 출력이 다소 정규화
            -원점주위로 몰리는 경향이 있음
        -ReLU 함수
            -계산속도 빠름. 
            -출력에 최댓값 없음 
    -출력층
        -소프트맥스 활성화 함수


MNIST 데이터 세트
    -load_data() 함수 이용하여 데이터 가져옴
    -손으로 쓴 글씨(0~9까지)의 이미지로 구성됨
    -훈련 데이터와 테스트 데이터를 반환
    -각각 입력 이미지와 해당 이미지의 레이블로 구성됨

astype() 함수를 사용하여 이미지 데이터 자료형 변경
    => 신경망 모델의 입력으로 사용하기 위함

2차원 배열에서 1차원 배열로 변환(전처리)
    -MNIST 데이터셋 이미지: 각각 28 * 28 크기의 2차원 배열임
    -신경망 모델: 입력으로 사용하기 위해서는 1차원 배열 형태로 변환해야 함
    - -1: 행의 크기는 자동으로 결정
      784: 열의 크기로 지정
    -x_train이 60000개 이미지이고, 각 이미지 크기가 28 * 28 이라면 
     reshape([-1, 784])를 적용하면 60000 * 784 크기의 2차원 배열로 변환됨
        -각 행은 하나의 이미지에 해당됨
        -픽셀 값들이 일렬로 나열된 형태로 저장됨

* MNIST 데이터 세트의 이미지는 픽셀 값(0~255)
    -이미지 데이터를 신경망 모델에 입력으로 사용할 때, 
     일반적으로 픽셀값 범위를 0부터 1사이로 제한하는 것이 좋음
     => 가중치 업데이트와 같은 연산에서도 안정성을 높여줌

one_hot은 다중 클래스 분류 문제를 해결하기 위함
    -신경망 모델에서 다중 클래스 분류를 수행하기 위해서는 출력값을 적절하게 설정해야 함
    -각 클래스에 대한 확률을 출력하기 위해 범주형 데이터 형식인 one-hot 인코딩을 사용함
        
    -각 클래스를 해당하는 인덱스에 1을 표시하고, 나머지 인덱스는 0을 표시하는 방식
        -각 클래스의 레이블을 벡터 형태로 표시함
        -변환된 레이블 벡터는 신경망 모델의 출력과 비교하여 정확성을 개선하거나 손실함수를 계산하는 작업에 사용됨

* 학습률
    -경사 하강법(gradient descent) 알고리즘에서 한번에 업데이트되는 가중치의 크기를 조절하는 역할을 함
        -학습률은 0~1 사이의 값으로 설정됨
        -너무 적으면 학습 속도가 느려짐, 너무 크면 문제가 됨
            
* 학습 횟수
    -전체 데이터 세트를 몇 번 반복하여 학습할지를 나타냄
    -너무 큰 값은 과적합(overfitting)의 가능성을 증가시킴
        
* 배치 개수
    -한 번에 모든 데이터를 처리하는 것이 아니라 일부 데이터를 묶어서 처리함 ==> 메모리 사용량을 줄이고, 계산 속도 향상시킴
        
* display_step
    -학습 과정에서 손실 함수의 값은 주로 모니터링 지표 중 하나임
    -몇 번째 학습 단게마다 손실 함수의 값을 출력할지를 결정함
    - 1: 매 학습 단게마다 출력됨
      
* input_size
    - 입력의 크기는 이미지의 특징 개수임
    - MNIST 데이터 세트는 이미지 크기가 28 * 28 = 784 픽셀임
        
* hidden1_size
    -은닉층(hidden layer)의 크기
    -신경망 모델의 은닉층은 입력과 출력 사이에 있는 층으로, 중간에 특성을 학습하고 추출하는 역할을 함
        
* output_size
    -모델의 출력층 노드의 크기
    -숫자 0부터 9까지 총 10개의 클래스를 분류(다중 클래스 분류)

* from_tensor_slices()
    -텐서들을 슬라이스하여 데이터셋을 생성하는 함수
    -이미지와 해당 레이블로 이루어진 훈련 데이터
        -묶어서 데이터셋을 생성함
* shuffler()
    -데이터셋을 섞는 역할
    -훈련 데이터를 무작위로 섞음
        -모델이 데이터의 순서에 의존하지 않고 학습되도록 하기 위함
* batch()
    -데이터셋을 배치로 나누는 역할
    -한번의 업데이트 단계에서 여러 개의 데이터 샘플을 함께 처리할 수 있음
    -배치처리를 통해 메모리 효율성과 계산 속도 개선함

* 네트웍의 가중치와 편향 변수 초기화
    -tf.Variable()으로 정의
    -첫번째 은닉층의 가중치와 편향

* return logits
    - 신경망의 예측값
    
* 데코레이터(decorator)
    -파이썬에서 함수나 클래스를 수정하거나 기능을 추가할 때 사용되는 문법적인 요소임
    -기존의 함수나 클래스를 변경하지 않으면서 새로운 기능을 추가함
* TensorFlow에서의 @tf.function 데코레이터
    -그래프로 함수를 변환하여 성능을 향상시키는 역할을 함
    -반복적인 연산을 줄여서 모델의 실행 속도를 개선함

* reduce_mean()
    -평균 손실은 모델의 전체 손실을 나타내는 지표로 사용됨
    -텐서의 손실 평균을 계산하는 함수임
        -교차 엔트로피 손실의 평균을 계산해서 반환함
        -예측값 logits와 실제 label y 사이의 교차 엔트로피 손실 계산
* Adam
    1) 최적화 알고리즘을 구현한 TensorFlow의 옵티마이저 클래스임
    2) 자동 미분을 활용하여 모델의 가중치를 업데이트하는데 사용되는 알고리즘임
    3) 학습률 - 각 파라미터 업데이트의 크기를 결정하는 요소

train_step() 함수
    1) 모델, 입력 데이터 x, 실제 레이블 y를 사용하여 한단계의 학습을 수행하고, 모델의 변수들을 업데이트하는 역할을 함
    2) tf.GradientTape()
        -블록 안에서의 모델의 연산들을 기록함
        -입력 데이터x를 받아서 예측값 y_pred을 계산함
        -이것을 손실함수에 적용하여 손실 loss를 계산함
    3) tape.gradient()
        -손실에 대한 모델의 변수들의 gradient(기울기)를 계산함
    4) optimizer.apply_gradients()
        -계산된 gradients(기울기)를 사용하여 모델의 변수들을 업데이트함
        -optimizer에 의해 gradient 기반 최적화 알고리즘을 수행

* 예측값 y_pred와 실제 label y를 사용하여 모델의 정확도를 계산하는 역할
=>모델의 성능을 평가하고 모델의 예측 결과를 정량적으로 측정할 수 있음   
    -tf.equal()
        -tf.argmax(): 가장 큰 원소의 인덱스를 가져옴
        -예측값과 실제 레이블이 일치하는지 여부를 계산함
            -일치하면 True 반환, 일치하지 않으면 False 반환
    -tf.cast() 
        -데이터 타입을 float32 타입으로 변경
        -True를 1.0으로 False를 0.0으로 변환(정확도 계산하기 위함)
