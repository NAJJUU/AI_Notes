1. 신경망(Neural Network) 개요
    1) 인간 두뇌의 뉴런의 작용을 모방한 모델
    2) 인공신경망(Artificial)이라 부르기도 함
    3) 신경망은 병렬성(parallelism)이 뛰어남
    4) 문자인식, 음성인식, 영상인식, 자연어 처리 등에 이용
    5) 학습과 관련된 지능적인 역할을 훌륭하게 수행 

2. 신경망의 학습(Learning)
    1) 모든 신경망의 공통적인 주 역할은 학습 기능임
    2) 데이터가 크고 다루기가 매우 어려운 것을 학습하고 인식
    3) 연결 강도(w) 조정하며 학습 
        -더 이상 조정이 없을 때까지 반복 수행

3. 초기의 단층 퍼셉트론 개발 => 다층 퍼셉트론 모델(Multi-layer)
    1) 단층 퍼셉트론에 은닉층을 첨가한 다층 퍼셉트론
    2) 다층 퍼셉트론 모델의 학습을 위한 역전파(Back-propagation) 알고리즘 발표
        -신경망의 새로운 시대 전개
    3) 어떤 명제도 AND, OR, NOT의 결합으로 표현 가능
        -한 직선에 의해 두 개 영역으로 분리 
    4) XOR 함수와 선형 분리 불가능
        -하나의 직선이 아닌 곡선에 의해서만 분리가 가능 
        -한 직선으로 두 집합을 교차하지 않고 나눌 수 없음
            -이 점은 단층 퍼셉트론 학습에서 매우 심각한 문제점
    5) 두 뉴런 사이의 연결강도 W가 존재
    6) n개의 입력과 n개의 연결강도(w) 벡터가 각각 곱해진 결과의 합이 활성 함수(activation function)에 의해 판단됨 

4. 다층 퍼셉트론(MLP)
    1) 단층 퍼셉트론 모델에 하나 이상의 은닉층을 추가로 사용
        -입력층과 출력층 사이에 하나 이상의 은닉층응 사용함
        -입력층, 은닉층, 출력층의 순서와 방향으로 연결
        -전방향과 역방향으로 반복적으로 움직이며 역전파 학습 
    2) 역전파(Backpropagation) 알고리즘 제안
        -다층 퍼셉트론 구조에 역전파 알고리즘 사용
        -XOR 함수의 선형 분리 문제 등 해결
        -입력층에서 은닉층을 거쳐 출력층, 다시 반대 방향으로 되돌아오면서 학습 
    3) 역전파 학습 알고리즘
        -입력층의 각 노드에 입력 패턴을 줌
        -이 신호는 각 노드에서 변환되어 은닉층에 전달
        -은닉층에서 출력층으로 신호 출력 
        -이 출력값과 기대하는 목표출력값을 비교 
            -그 차이를 감소시키는 방향으로 계속 연결강도(W) 조정 
        -다시 역전파하여 해당 층들의 연결강도(W) 조정 
        -출력값과 목표출력값이 오차 범위 내이면 학습 완료!
            -그렇지 않으면 역전파 과정으로 연결강도(W) 조정 반복 
    4) 다층 퍼셉트론의 학습 과정과 규칙들
        -경사하강법(Gradient descent method)
            -곡면에서 오차의 제곱이 가장 많이 감소하는 방향(Global Minimum, 전역최소점)으로 기울기(W)를 따라가며 변화하는 방법
            -ex) 아담(Adaptive Moment Estimation, Adam)
        -지역최소점(Local Minimum) 문제 
            -역전파 알고리즘의 단점 
            -오랜 학습 시간, 낮은 확률이나 지역 최소점 봉착 가능
        -지역최소점이 아닌 전역최소점을 추구